{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 100973,
          "databundleVersionId": 12202417,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TASK DESCRIPTION"
      ],
      "metadata": {
        "id": "BO1sJik47Aps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Legend:**\n",
        "\n",
        "Young Alex has a beloved BERT model that he carries everywhere on his trusty flash drive. One day, during an excursion along the River Styx, a few drops of water landed on the precious device, corrupting the model's weights.\n",
        "\n",
        "Heartbroken, Alex rushed home to fix the neural network. After quick analysis, he discovered only the token embeddings were damaged - the rest of the architecture (attention blocks and heads) remained perfectly intact. Now he needs to restore the model's performance on Sentiment Analysis Task.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "You need to fix the broken vectors of the Embeddings matrix of the model so as to improve the quality of the model on the task of text sentiment analysis.\n",
        "\n",
        "**Restrictions:**\n",
        "\n",
        "- You can not use any other transformer based pre-trained models and LLMs.\n",
        "\n",
        "- You can not any additional data\n",
        "\n",
        "- You can not fine-tune or pre-train model\n",
        "\n",
        "===\n",
        "\n",
        "When you make a submit, make a Quick Save of the notebook, otherwise we may reject your solution.\n",
        "\n",
        "You must solve this task on KAGGLE (YOU CAN'T USE CLOUD.RU)\n",
        "\n",
        "==========\n",
        "\n",
        "**Легенда:**\n",
        "\n",
        "Young Alex имеет любимую модель BERT, которую он везде носит на своей надежной флешке. Однажды, во время экскурсии вдоль реки Стикс, несколько капель воды попало на драгоценное устройство, повредив веса модели.\n",
        "\n",
        "С разбитым сердцем Алекс поспешил домой, чтобы починить нейронную сеть. После быстрого анализа он обнаружил, что повреждены только эмбеддинги токенов — остальная архитектура (блоки внимания и головы) осталась полностью нетронутой.\n",
        "\n",
        "Теперь ему нужно восстановить производительность модели, оставив все остальные веса замороженными (никакие изменения в механизмах внимания или других компонентах не допускаются). Ваша задача — помочь Алексу достичь этой цели, не нарушив его ностальгическую привязанность к оригинальной модели.\n",
        "\n",
        "**Задача:**\n",
        "\n",
        "Вам необходимо починить сломанные вектора матрицы Embeddings модели так, чтобы улучшить качество модели на задаче анализа тональности текста.\n",
        "\n",
        "**Ограничения:**\n",
        "\n",
        "- Вы не можете использовать никакие другие предобученные модели на основе архитектуры Трансформер и LLM.\n",
        "\n",
        "- Вы не можете использовать никакие дополнительные данные.\n",
        "\n",
        "- Вы не можете дообучать или предобучать модель.\n",
        "\n",
        "===\n",
        "\n",
        "При отправке решения сделайте Quick Save ноутбука, иначе мы можем отклонить ваше решение.\n",
        "\n",
        "Эту задачу необходимо решить на KAGGLE (ВЫ НЕ МОЖЕТЕ ИСПОЛЬЗОВАТЬ CLOUD.RU)\n"
      ],
      "metadata": {
        "id": "q1TYuiZy7Apu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEPENDINGS"
      ],
      "metadata": {
        "id": "S3zzpVND7Apw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "np.random.seed(21)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-08T06:29:57.482916Z",
          "iopub.execute_input": "2025-05-08T06:29:57.483505Z",
          "iopub.status.idle": "2025-05-08T06:29:57.487550Z",
          "shell.execute_reply.started": "2025-05-08T06:29:57.483479Z",
          "shell.execute_reply": "2025-05-08T06:29:57.486583Z"
        },
        "id": "sTKsjIbQ7Apx"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD DATASET"
      ],
      "metadata": {
        "id": "wDXX8T9x7Apy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload kaggle.json file to colab."
      ],
      "metadata": {
        "id": "Fb-qWqjZ-_Iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06dnTOsL-uO1",
        "outputId": "532ff79b-7ac3-4dde-9e46-abff807d3d63"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c neoai-2025-broken-bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz0Mw6hq7qBz",
        "outputId": "8e5244a9-d664-494d-de7e-cddec5e863ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neoai-2025-broken-bert.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip neoai-2025-broken-bert.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_dmwov7_G9z",
        "outputId": "3d35f0dd-05fc-4c75-9306-2f5307194d29"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  neoai-2025-broken-bert.zip\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: test.csv                \n",
            "  inflating: val_dataset.csv         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_data_path = \"val_dataset.csv\"\n",
        "test_data_path = \"test.csv\"\n",
        "\n",
        "val_df = pd.read_csv(val_data_path)\n",
        "\n",
        "test_df = pd.read_csv(test_data_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-08T06:30:00.398657Z",
          "iopub.execute_input": "2025-05-08T06:30:00.398997Z",
          "iopub.status.idle": "2025-05-08T06:30:00.436368Z",
          "shell.execute_reply.started": "2025-05-08T06:30:00.398973Z",
          "shell.execute_reply": "2025-05-08T06:30:00.435541Z"
        },
        "id": "M4QcHSE67Apz"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD TOKENIZER & MODEL"
      ],
      "metadata": {
        "id": "mZSZB2mU7Apz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Ilseyar-kfu/broken_bert\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-08T06:29:11.224391Z",
          "iopub.execute_input": "2025-05-08T06:29:11.224920Z",
          "iopub.status.idle": "2025-05-08T06:29:11.887397Z",
          "shell.execute_reply.started": "2025-05-08T06:29:11.224894Z",
          "shell.execute_reply": "2025-05-08T06:29:11.886764Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7-QpFBE7Apz",
        "outputId": "9513339a-be70-49f9-bd0d-83b3190fcea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-07T21:37:22.728808Z",
          "iopub.execute_input": "2025-05-07T21:37:22.729139Z",
          "iopub.status.idle": "2025-05-07T21:37:22.734971Z",
          "shell.execute_reply.started": "2025-05-07T21:37:22.729113Z",
          "shell.execute_reply": "2025-05-07T21:37:22.734212Z"
        },
        "id": "8GtjYkit7Ap0"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "val_encodings = tokenizer(val_df[\"text\"].to_list(), truncation=True, padding=True, max_length=256)\n",
        "val_dataset = Dataset(val_encodings, val_df[\"labels\"].to_list())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-07T17:25:58.248359Z",
          "iopub.execute_input": "2025-05-07T17:25:58.248667Z",
          "iopub.status.idle": "2025-05-07T17:25:58.379828Z",
          "shell.execute_reply.started": "2025-05-07T17:25:58.248603Z",
          "shell.execute_reply": "2025-05-07T17:25:58.379273Z"
        },
        "id": "L6DFAiSs7Ap0"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "texts_2_score = val_df[\"text\"].to_list() + test_df[\"text\"].to_list()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-07T17:25:58.38195Z",
          "iopub.execute_input": "2025-05-07T17:25:58.382144Z",
          "iopub.status.idle": "2025-05-07T17:25:58.386012Z",
          "shell.execute_reply.started": "2025-05-07T17:25:58.382129Z",
          "shell.execute_reply": "2025-05-07T17:25:58.385474Z"
        },
        "id": "XmQYDHrk7Ap1"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL CHANGES"
      ],
      "metadata": {
        "id": "Ys16pqGe7Ap1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution: get average sub tokens' embeddings for each token with zero embedding."
      ],
      "metadata": {
        "id": "PRk7t2Sp_S_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get tokens with zero indices"
      ],
      "metadata": {
        "id": "Xyl0HO2rSj9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "n0JeUkLKZMdz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get token indices with zero embeddings."
      ],
      "metadata": {
        "id": "1FCcchy0e2JE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_embeddings = model.bert.embeddings.word_embeddings.weight.detach().cpu()\n",
        "zero_rows = (model_embeddings == 0).all(dim=1)\n",
        "zero_indices = torch.nonzero(zero_rows).squeeze().numpy()"
      ],
      "metadata": {
        "id": "Clz3pGnyBzgC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get tokenizer vocabulary:"
      ],
      "metadata": {
        "id": "YyOZ-pm2fKON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_to_ids = tokenizer.get_vocab()\n",
        "ids_to_token = {v: k for k,v in token_to_ids.items()}\n",
        "non_zero_ids_to_token = {v: k for k,v in token_to_ids.items() if v not in zero_indices}"
      ],
      "metadata": {
        "id": "m2JIVzJpSjCY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to get all tokens from the tokenizer vocabulary that are included in the current token."
      ],
      "metadata": {
        "id": "2402oNOQfHPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sub_tokens(token, ids_to_token):\n",
        "  sub_tokens = []\n",
        "  for idx, sub_token in ids_to_token.items():\n",
        "    if sub_token in token:\n",
        "      sub_tokens.append(idx)\n",
        "  return sub_tokens"
      ],
      "metadata": {
        "id": "idV_zjrNWg3o"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each token with zero embeddings, we find subtokens from the tokenizer vocabulary and replace the vectors of these tokens with the averaged embeddings of the subtokens:"
      ],
      "metadata": {
        "id": "bIXvIHp8f-Pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for zero_index in tqdm(zero_indices):\n",
        "  zero_token = ids_to_token[zero_index]\n",
        "  sub_tokens = get_sub_tokens(zero_token, non_zero_ids_to_token)\n",
        "  if len(sub_tokens) != 0:\n",
        "    mean_embedding = model_embeddings[sub_tokens].mean(dim=0)\n",
        "    model_embeddings[zero_index] = mean_embedding\n",
        "  else:\n",
        "    model_embeddings[zero_index] = torch.rand(1, model_embeddings.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF9Zx7HWWXSg",
        "outputId": "f158b973-6a4d-4d85-e5f4-8d44d19ca954"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12208/12208 [00:23<00:00, 523.20it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace the model's embedding matrix with a new emedding matrix"
      ],
      "metadata": {
        "id": "UP7B4uIOgDUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_embedings = model_embeddings"
      ],
      "metadata": {
        "id": "QLs9YENmbtJW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"Ilseyar-kfu/broken_bert\")\n",
        "\n",
        "# There's magic going on here!!! And we get very new !!! new_embedings !!!\n",
        "\n",
        "model.bert.embeddings.word_embeddings.weight = torch.nn.Parameter(torch.Tensor(new_embedings))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-08T06:30:04.904445Z",
          "iopub.execute_input": "2025-05-08T06:30:04.904797Z",
          "iopub.status.idle": "2025-05-08T06:30:05.104852Z",
          "shell.execute_reply.started": "2025-05-08T06:30:04.904772Z",
          "shell.execute_reply": "2025-05-08T06:30:05.103965Z"
        },
        "id": "HMc4MnKM7Ap1"
      },
      "outputs": [],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATION"
      ],
      "metadata": {
        "id": "3zswDsP67Ap1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from numpy import argmax\n",
        "from transformers import pipeline\n",
        "import wandb\n",
        "wandb.init(mode= \"disabled\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-08T06:30:25.953411Z",
          "iopub.execute_input": "2025-05-08T06:30:25.953970Z",
          "iopub.status.idle": "2025-05-08T06:30:34.168679Z",
          "shell.execute_reply.started": "2025-05-08T06:30:25.953946Z",
          "shell.execute_reply": "2025-05-08T06:30:34.167900Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "s4lc9u0C7Ap2",
        "outputId": "14e05c45-95da-4789-edc1-66b8300cb918"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/fluu8pk1?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c73530e0e10>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate_on_validation(model, tokenizer, df_val):\n",
        "    label_2_dict = {'LABEL_0': 'neutral', \"LABEL_1\" : 'positive', \"LABEL_2\": 'negative'}\n",
        "    classifier = pipeline(\"text-classification\", model= model, tokenizer = tokenizer)\n",
        "    answ = classifier.predict(list(df_val[\"text\"]))\n",
        "    answ = [label_2_dict[el[\"label\"]] for el in answ]\n",
        "\n",
        "    # print(f1_score(p.label_ids, preds, average='macro'))\n",
        "    print(classification_report(df_val[\"labels\"], answ))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-08T06:29:44.794184Z",
          "iopub.execute_input": "2025-05-08T06:29:44.795192Z",
          "iopub.status.idle": "2025-05-08T06:29:44.803778Z",
          "shell.execute_reply.started": "2025-05-08T06:29:44.795147Z",
          "shell.execute_reply": "2025-05-08T06:29:44.802675Z"
        },
        "id": "WmuYU1mF7Ap2"
      },
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_on_validation(model, tokenizer, val_df)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt06Izcz7Ap2",
        "outputId": "11671f4d-b9fa-45d2-93d8-fa40b2792bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.72      0.13      0.22       935\n",
            "     neutral       0.33      0.87      0.48       759\n",
            "    positive       0.57      0.22      0.32       806\n",
            "\n",
            "    accuracy                           0.39      2500\n",
            "   macro avg       0.54      0.41      0.34      2500\n",
            "weighted avg       0.55      0.39      0.33      2500\n",
            "\n"
          ]
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL SCORING\n",
        "When you make a submit,\n",
        "1. Make a Quick Save of the notebook, otherwise we may reject your solution!\n",
        "2. Add notebook version to the comment for the submit.\n",
        "\n",
        "===\n",
        "\n",
        "При отправке решения:\n",
        "\n",
        "1. Сделайте Quick Save ноутбука, иначе мы можем отклонить ваше решение!\n",
        "2. Добавьте версию ноутбука в комментарий к отправке."
      ],
      "metadata": {
        "id": "2zgRxqrT7Ap3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "def create_submission(model, tokenizer, df_test):\n",
        "    label_2_dict = {'LABEL_0': 'neutral', \"LABEL_1\" : 'positive', \"LABEL_2\": 'negative'}\n",
        "    classifier = pipeline(\"text-classification\", model= model, tokenizer = tokenizer)\n",
        "    answ = classifier.predict(list(df_test[\"text\"]))\n",
        "    answ = [label_2_dict[el[\"label\"]] for el in answ]\n",
        "\n",
        "    df = pd.DataFrame({\"labels\" : answ, \"id\": df_test['id']})\n",
        "    hsh = hashlib.sha256(df.to_csv(index=False).encode('utf-8')).hexdigest()[:8]\n",
        "    submit_path = f\"submit_{hsh}.csv\"\n",
        "    print(f\"SUBMIT_NAME: {submit_path}\")\n",
        "    print(df.head(10))\n",
        "    df.to_csv(submit_path,index=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-08T06:31:21.327431Z",
          "iopub.execute_input": "2025-05-08T06:31:21.327789Z",
          "iopub.status.idle": "2025-05-08T06:31:21.334045Z",
          "shell.execute_reply.started": "2025-05-08T06:31:21.327763Z",
          "shell.execute_reply": "2025-05-08T06:31:21.333183Z"
        },
        "id": "gAxn7UId7Ap3"
      },
      "outputs": [],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": [
        "create_submission(model, tokenizer, test_df)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbCjmH1I7Ap3",
        "outputId": "ba7027e6-b2e9-4dae-f83b-e7dd4d54d829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUBMIT_NAME: submit_f5fc7d7e.csv\n",
            "     labels    id\n",
            "0  positive  5000\n",
            "1   neutral  5001\n",
            "2   neutral  5002\n",
            "3   neutral  5003\n",
            "4   neutral  5004\n",
            "5   neutral  5005\n",
            "6   neutral  5006\n",
            "7   neutral  5007\n",
            "8   neutral  5008\n",
            "9   neutral  5009\n"
          ]
        }
      ],
      "execution_count": 25
    }
  ]
}